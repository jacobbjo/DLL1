import matplotlib.pyplot as plt
import numpy as np


def unpickle(file):
    import pickle
    with open(file, 'rb') as fo:
        dict = pickle.load(fo, encoding='bytes')
    return dict


def readfile(file):
    file_dict = unpickle(file)
    data = file_dict[b"data"]
    labels = file_dict[b"labels"]

    # Generate one hot representation of the labels:
    one_hots = np.zeros((10, len(labels)))

    for ind, label in enumerate(labels):
        one_hots[label, ind] = 1

    return np.transpose(data/255), one_hots, np.array(labels)


def softmax(s):
    return np.exp(s)/np.sum(np.exp(s), axis=0)


def evaluate_classifier(X, W, b):
    s = np.matmul(W, X) + b
    P = softmax(s)
    return P


def compute_cost(X, Y, W, b, lamb):
    """the sum of the loss of the networkâ€™s predictions for the images in X relative to
    the ground truth labels and the regularization term on W"""

    P = evaluate_classifier(X, W, b)
    cross = -1/X.shape[1] * np.sum(np.log(np.matmul(np.transpose(Y), P)))
    J = cross + lamb * np.sum(np.power(W, 2))
    return J


def compute_accuracy(X, y, W, b):
    P = evaluate_classifier(X, W, b)
    preds = np.argmax(P, axis=0) #the predicted class for each picture
    are_equal = np.sum(preds == y)
    return are_equal/preds.shape[0]


def compute_gradients(X, Y, P, W, lamb):
    # From lecture 3 we can deduct that we need:
    # ğ›¿J/ğ›¿b =
    # ğ›¿J/ğ›¿s * ğ›¿s/ğ›¿b =
    # ğ›¿J/ğ›¿p * ğ›¿p/ğ›¿s * ğ›¿s/ğ›¿b =
    # ğ›¿J/ğ›¿l * ğ›¿l/ğ›¿p * ğ›¿p/ğ›¿s * ğ›¿s/ğ›¿b =
    # ğ›¿J/ğ›¿J * ğ›¿J/ğ›¿l * ğ›¿l/ğ›¿p * ğ›¿p/ğ›¿s * ğ›¿s/ğ›¿b

    # And:
    # ğ›¿J/ğ›¿W =
    # ğ›¿J/ğ›¿z * ğ›¿z/ğ›¿W + ğ›¿J/ğ›¿r * ğ›¿r/ğ›¿w =
    # ğ›¿J/ğ›¿s * ğ›¿s/ğ›¿z * ğ›¿z/ğ›¿W + ğ›¿J/ğ›¿J * ğ›¿J/ğ›¿r * ğ›¿r/ğ›¿W

    # So:

    djdj = 1

    djdl = 1 #*djdj

    dldp = np.sum((Y * (-1/P)), axis=1)




    djdr = lamb


    print("lol")


def disp_img(image):
    plt.imshow(np.transpose(image.reshape(3, 32, 32), (1, 2, 0)))
    plt.show()

